{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    t = 0\n",
    "    env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        t += 1\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Monte-Carlo 策略梯度算法\n",
    "\"\"\"\n",
    "\n",
    "def mc_policy_gradient(env, theta, lr, episodes):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    env -- 环境\n",
    "    theta -- 参数\n",
    "    lr -- 学习率\n",
    "    episodes -- 迭代次数\n",
    "\n",
    "    返回: \n",
    "    episodes -- 参数\n",
    "    \"\"\"\n",
    "    for episode in range(episodes):  # 迭代 episode\n",
    "        episode = []\n",
    "        start_observation = env.reset()  # 初始化环境\n",
    "        t = 0\n",
    "        while True:\n",
    "            policy = np.dot(theta, start_observation)  # 计算策略值\n",
    "            # 这里的 action_space 为 2, 故使用 Sigmoid 激活函数处理策略值\n",
    "            pi = 1 / (1 + np.exp(-policy))\n",
    "            if pi >= 0.5:\n",
    "                action = 1  # 向右施加力\n",
    "            else:\n",
    "                action = 0  # 向左施加力\n",
    "            next_observation, reward, done, _ = env.step(action)  # 执行动作\n",
    "            # 将环境返回结果添加到 episode 中\n",
    "            episode.append([next_observation, action, pi, reward])\n",
    "            start_observation = next_observation  # 将返回 observation 作为下一次迭代 observation\n",
    "            t += 1\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t))\n",
    "                break\n",
    "        # 根据上一次 episode 更新参数 theta\n",
    "        for timestep in episode:\n",
    "            observation, action, pi, reward = timestep\n",
    "            theta += lr * (1 - pi) * np.transpose(-observation) * reward\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lr = 0.001\n",
    "theta = np.random.rand(4)\n",
    "episodes=10\n",
    "\n",
    "mc_policy_gradient(env, theta, lr, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Actor-Critic 策略梯度算法\n",
    "\"\"\"\n",
    "def ac_policy_gradient(env, theta, w, lr, gamma, episodes):\n",
    "    done = True\n",
    "    for _ in range(episodes):\n",
    "        t = 0\n",
    "        while True:\n",
    "            if done:  # 根据 done 值判断是否重新设定环境\n",
    "                start_observation = env.reset()  # 初始化环境\n",
    "                # 根据策略选择相应的动作\n",
    "                policy = np.dot(theta, start_observation)\n",
    "                start_pi = 1 / (1 + np.exp(-policy))\n",
    "                if start_pi >= 0.5:\n",
    "                    start_action = 1\n",
    "                else:\n",
    "                    start_action = 0\n",
    "                start_q = np.dot(w, start_observation)  # 计算价值 Q\n",
    "\n",
    "            observation, reward, done, _ = env.step(start_action)  # 执行动作\n",
    "            # 根据新策略选择相应的动作\n",
    "            policy = np.dot(theta, observation)\n",
    "            pi = 1 / (1 + np.exp(-policy))\n",
    "            if pi >= 0.5:\n",
    "                action = 1\n",
    "            else:\n",
    "                action = 0\n",
    "            q = np.dot(w, observation)\n",
    "            # 更新参数\n",
    "            delta = reward + gamma * q - start_q\n",
    "            theta += lr * (1 - start_pi) * np.transpose(-start_observation) * start_q\n",
    "            w += lr * delta * np.transpose(start_observation)\n",
    "            start_pi, start_observation, start_q, start_action = pi, observation, q, action\n",
    "\n",
    "            t += 1\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break\n",
    "    return theta, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "theta = np.random.rand(4)\n",
    "w = np.random.rand(4)\n",
    "lr = 0.001\n",
    "episodes=10\n",
    "\n",
    "ac_policy_gradient(env, theta, w, lr, gamma, episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
